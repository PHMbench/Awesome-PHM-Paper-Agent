@article{zhang2025sckd,
  title={Class Incremental Fault Diagnosis under Limited Fault Data via Supervised Contrastive Knowledge Distillation},
  author={Zhang, Yuqing and Li, Wei and Chen, Hongyang and Wang, Junfeng},
  journal={arXiv preprint arXiv:2501.09525},
  year={2025},
  month={January},
  eprint={2501.09525},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  keywords={Continual learning, Class incremental learning, Fault diagnosis, Knowledge distillation, Contrastive learning, Limited data, Catastrophic forgetting},
  abstract={This paper proposes a novel Supervised Contrastive Knowledge Distillation (SCKD) approach for class incremental fault diagnosis under limited fault data. The method employs a Marginal Exemplar Selection (MES) strategy that prioritizes samples near decision boundaries, effectively mitigating catastrophic forgetting in class incremental scenarios.},
  note={Recent breakthrough in continual learning for industrial fault diagnosis with limited data scenarios},
  url={https://arxiv.org/abs/2501.09525}
}