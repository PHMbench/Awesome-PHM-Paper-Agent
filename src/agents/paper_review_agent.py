"""
Paper Review Agent - è®ºæ–‡å®¡æŸ¥ä»£ç†

è¿™ä¸ªä»£ç†è´Ÿè´£éªŒè¯æ–°å¢è®ºæ–‡çš„çœŸå®æ€§ï¼Œç¡®ä¿æ‰€æœ‰æ·»åŠ åˆ°çŸ¥è¯†åº“çš„è®ºæ–‡éƒ½æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œ
è€Œä¸æ˜¯AIç”Ÿæˆçš„è™šå‡å†…å®¹ã€‚å®ƒä¼šæ£€æŸ¥DOIã€éªŒè¯ä½œè€…ä¿¡æ¯ã€ç¡®è®¤æ‘˜è¦çœŸå®æ€§ç­‰ã€‚
"""

import re
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from urllib.parse import urlparse

from .base_agent import BaseAgent, AgentError
from ..utils.logging_config import get_logger


class PaperReviewAgent(BaseAgent):
    """
    è®ºæ–‡å®¡æŸ¥ä»£ç†
    
    éªŒè¯è®ºæ–‡çš„çœŸå®æ€§ï¼ŒåŒ…æ‹¬ï¼š
    - DOI æœ‰æ•ˆæ€§éªŒè¯
    - ä½œè€…å’Œæœºæ„ä¿¡æ¯æ£€æŸ¥
    - æ‘˜è¦çœŸå®æ€§è¯„ä¼°
    - å­¦æœ¯æ•°æ®åº“äº¤å‰éªŒè¯
    - å†…å®¹è´¨é‡è¯„ä¼°
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config, "PaperReviewAgent")
        
        # å®¡æŸ¥é…ç½®
        self.review_config = self.get_config_value('paper_review_settings', {})
        self.strict_mode = self.review_config.get('strict_mode', True)
        self.require_doi = self.review_config.get('require_doi', False)
        self.min_abstract_length = self.review_config.get('min_abstract_length', 100)
        self.max_abstract_length = self.review_config.get('max_abstract_length', 2000)
        
        # å¯ä¿¡çš„å­¦æœ¯åŸŸå
        self.trusted_domains = {
            'arxiv.org',
            'ieeexplore.ieee.org',
            'pubmed.ncbi.nlm.nih.gov',
            'scholar.google.com',
            'semanticscholar.org',
            'acm.org',
            'springer.com',
            'elsevier.com',
            'wiley.com',
            'nature.com',
            'science.org'
        }
        
        # PHM ç›¸å…³å…³é”®è¯ (æ›´æ–°äº†LLMç›¸å…³æœ¯è¯­)
        self.phm_keywords = {
            'core': [
                'prognostics', 'health management', 'predictive maintenance',
                'condition monitoring', 'fault diagnosis', 'remaining useful life',
                'RUL', 'PHM', 'condition based maintenance', 'CBM'
            ],
            'technical': [
                'vibration analysis', 'signal processing', 'feature extraction',
                'machine learning', 'deep learning', 'neural networks',
                'anomaly detection', 'pattern recognition', 'time series analysis',
                'large language model', 'LLM', 'knowledge graph', 'transformer',
                'ChatGPT', 'foundation model', 'fine-tuning', 'prompt engineering'
            ],
            'applications': [
                'bearing', 'gearbox', 'motor', 'turbine', 'machinery',
                'rotating equipment', 'mechanical systems', 'industrial equipment',
                'aviation', 'aerospace', 'manufacturing', 'assembly'
            ]
        }
        
        # è´¨é‡è¯„åˆ†æƒé‡
        self.quality_weights = {
            'venue_reputation': 0.3,      # æœŸåˆŠ/ä¼šè®®å£°èª‰
            'phm_relevance': 0.25,        # PHMç›¸å…³æ€§
            'content_quality': 0.2,       # å†…å®¹è´¨é‡
            'author_credibility': 0.15,   # ä½œè€…å¯ä¿¡åº¦
            'novelty_impact': 0.1         # åˆ›æ–°æ€§å’Œå½±å“åŠ›
        }
        
        # æœŸåˆŠç­‰çº§è¯„åˆ†
        self.venue_scores = {
            'IEEE Transactions on Industrial Informatics': 0.95,
            'IEEE Transactions on Reliability': 0.90,
            'Mechanical Systems and Signal Processing': 0.88,
            'Reliability Engineering & System Safety': 0.85,
            'Journal of Manufacturing Systems': 0.82,
            'Applied Soft Computing': 0.75,
            'Expert Systems with Applications': 0.72,
            'arXiv': 0.60,  # é¢„å°æœ¬åŸºç¡€åˆ†
            'unknown': 0.40
        }
        
        self.logger.info("Paper Review Agent initialized")
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å®¡æŸ¥è®ºæ–‡åˆ—è¡¨çš„çœŸå®æ€§
        
        Args:
            input_data: åŒ…å«å¾…å®¡æŸ¥è®ºæ–‡åˆ—è¡¨çš„å­—å…¸
            
        Returns:
            å®¡æŸ¥ç»“æœï¼ŒåŒ…å«é€šè¿‡å’Œæœªé€šè¿‡çš„è®ºæ–‡
        """
        
        papers = input_data.get('papers', [])
        if not papers:
            raise AgentError("No papers provided for review")
        
        self.logger.info(f"Starting review of {len(papers)} papers")
        
        review_results = {
            'total_papers': len(papers),
            'approved_papers': [],
            'rejected_papers': [],
            'warnings': [],
            'review_summary': {},
            'review_date': datetime.now().isoformat()
        }
        
        for i, paper in enumerate(papers, 1):
            self.logger.info(f"Reviewing paper {i}/{len(papers)}: {paper.get('title', 'Unknown')}")
            
            try:
                review_result = self._review_single_paper(paper)
                
                if review_result['approved']:
                    review_results['approved_papers'].append({
                        'paper': paper,
                        'review_score': review_result['score'],
                        'review_notes': review_result['notes']
                    })
                else:
                    review_results['rejected_papers'].append({
                        'paper': paper,
                        'rejection_reasons': review_result['rejection_reasons'],
                        'review_score': review_result['score']
                    })
                
                # æ”¶é›†è­¦å‘Š
                if review_result.get('warnings'):
                    review_results['warnings'].extend(review_result['warnings'])
                    
            except Exception as e:
                self.logger.error(f"Failed to review paper: {e}")
                review_results['rejected_papers'].append({
                    'paper': paper,
                    'rejection_reasons': [f"Review failed: {str(e)}"],
                    'review_score': 0.0
                })
        
        # ç”Ÿæˆå®¡æŸ¥æ‘˜è¦
        review_results['review_summary'] = self._generate_review_summary(review_results)
        
        self.logger.info(f"Review complete: {len(review_results['approved_papers'])} approved, {len(review_results['rejected_papers'])} rejected")
        
        return review_results
    
    def _review_single_paper(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """å®¡æŸ¥å•ç¯‡è®ºæ–‡"""
        
        review_result = {
            'approved': False,
            'score': 0.0,
            'notes': [],
            'warnings': [],
            'rejection_reasons': []
        }
        
        # 1. åŸºæœ¬ä¿¡æ¯éªŒè¯
        basic_score, basic_issues = self._validate_basic_info(paper)
        review_result['score'] += basic_score * 0.3
        
        if basic_issues:
            review_result['rejection_reasons'].extend(basic_issues)
        
        # 2. DOI éªŒè¯
        doi_score, doi_issues = self._validate_doi(paper)
        review_result['score'] += doi_score * 0.2
        
        if doi_issues:
            if self.require_doi:
                review_result['rejection_reasons'].extend(doi_issues)
            else:
                review_result['warnings'].extend(doi_issues)
        
        # 3. ä½œè€…ä¿¡æ¯éªŒè¯
        author_score, author_issues = self._validate_authors(paper)
        review_result['score'] += author_score * 0.15
        
        if author_issues:
            review_result['warnings'].extend(author_issues)
        
        # 4. æ‘˜è¦è´¨é‡éªŒè¯
        abstract_score, abstract_issues = self._validate_abstract(paper)
        review_result['score'] += abstract_score * 0.25
        
        if abstract_issues:
            review_result['rejection_reasons'].extend(abstract_issues)
        
        # 5. PHM ç›¸å…³æ€§éªŒè¯
        relevance_score, relevance_issues = self._validate_phm_relevance(paper)
        review_result['score'] += relevance_score * 0.1
        
        if relevance_issues:
            review_result['rejection_reasons'].extend(relevance_issues)
        
        # 6. æ•°æ®æºéªŒè¯
        source_score, source_issues = self._validate_source(paper)
        review_result['score'] += source_score * 0.1
        
        if source_issues:
            review_result['warnings'].extend(source_issues)
        
        # 7. ç»¼åˆè´¨é‡è¯„åˆ† (æ–°å¢)
        comprehensive_score = self._calculate_comprehensive_quality_score(paper)
        review_result['comprehensive_quality'] = comprehensive_score
        review_result['quality_breakdown'] = self._get_quality_breakdown(paper)
        
        # è°ƒæ•´æœ€ç»ˆè¯„åˆ† (ç»¼åˆè´¨é‡åˆ†æ•°çš„æƒé‡)
        final_score = review_result['score'] * 0.7 + comprehensive_score * 0.3
        review_result['final_score'] = final_score
        
        # å†³å®šæ˜¯å¦é€šè¿‡
        min_score = 0.7 if self.strict_mode else 0.5
        review_result['approved'] = (final_score >= min_score and 
                                   len(review_result['rejection_reasons']) == 0)
        
        # æ·»åŠ å®¡æŸ¥è¯´æ˜
        if review_result['approved']:
            review_result['notes'].append(f"Paper approved with final score {final_score:.2f} (basic: {review_result['score']:.2f}, quality: {comprehensive_score:.2f})")
        else:
            review_result['notes'].append(f"Paper rejected with final score {final_score:.2f}")
        
        return review_result
    
    def _validate_basic_info(self, paper: Dict[str, Any]) -> Tuple[float, List[str]]:
        """éªŒè¯åŸºæœ¬ä¿¡æ¯å®Œæ•´æ€§"""
        
        issues = []
        score = 0.0
        
        # æ£€æŸ¥æ ‡é¢˜
        title = paper.get('title', '').strip()
        if not title:
            issues.append("Missing paper title")
        elif len(title) < 10:
            issues.append("Paper title too short (< 10 characters)")
        elif len(title) > 200:
            issues.append("Paper title too long (> 200 characters)")
        else:
            score += 0.3
        
        # æ£€æŸ¥ä½œè€…
        authors = paper.get('authors', [])
        if not authors:
            issues.append("Missing authors information")
        elif len(authors) > 20:
            issues.append("Too many authors (> 20)")
        else:
            score += 0.2
        
        # æ£€æŸ¥å¹´ä»½
        year = paper.get('year')
        if not year:
            issues.append("Missing publication year")
        else:
            try:
                year_int = int(year)
                if year_int < 2000 or year_int > datetime.now().year + 1:
                    issues.append(f"Invalid publication year: {year}")
                else:
                    score += 0.2
            except:
                issues.append(f"Invalid year format: {year}")
        
        # æ£€æŸ¥æ‘˜è¦å­˜åœ¨æ€§
        abstract = paper.get('abstract', '').strip()
        if not abstract:
            issues.append("Missing abstract")
        else:
            score += 0.3
        
        return score, issues
    
    def _validate_doi(self, paper: Dict[str, Any]) -> Tuple[float, List[str]]:
        """éªŒè¯ DOI"""
        
        doi = paper.get('doi', '').strip()
        issues = []
        score = 0.0
        
        if not doi:
            issues.append("No DOI provided")
            return score, issues
        
        # DOI æ ¼å¼éªŒè¯
        doi_pattern = r'^10\.\d+/.+$'
        if not re.match(doi_pattern, doi):
            issues.append(f"Invalid DOI format: {doi}")
            return score, issues
        
        # DOI çœ‹èµ·æ¥æœ‰æ•ˆ
        score = 1.0
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨ WebFetch éªŒè¯ DOI å®é™…å­˜åœ¨æ€§
        # validation_url = f"https://doi.org/{doi}"
        # result = WebFetch(validation_url, "éªŒè¯DOIæ˜¯å¦å­˜åœ¨ï¼Œè¿”å›è®ºæ–‡æ ‡é¢˜")
        
        self.logger.info(f"DOI validation needed for: {doi}")
        
        return score, issues
    
    def _validate_authors(self, paper: Dict[str, Any]) -> Tuple[float, List[str]]:
        """éªŒè¯ä½œè€…ä¿¡æ¯"""
        
        authors = paper.get('authors', [])
        issues = []
        score = 0.0
        
        if not authors:
            issues.append("No authors provided")
            return score, issues
        
        valid_authors = 0
        
        for author in authors:
            if isinstance(author, str) and len(author.strip()) > 2:
                # æ£€æŸ¥ä½œè€…åç§°æ ¼å¼
                if re.match(r'^[A-Za-z\s\.,\-]+$', author.strip()):
                    valid_authors += 1
                else:
                    issues.append(f"Invalid author name format: {author}")
        
        if valid_authors > 0:
            score = min(1.0, valid_authors / len(authors))
        
        return score, issues
    
    def _validate_abstract(self, paper: Dict[str, Any]) -> Tuple[float, List[str]]:
        """éªŒè¯æ‘˜è¦è´¨é‡"""
        
        abstract = paper.get('abstract', '').strip()
        issues = []
        score = 0.0
        
        if not abstract:
            issues.append("No abstract provided")
            return score, issues
        
        # é•¿åº¦æ£€æŸ¥
        if len(abstract) < self.min_abstract_length:
            issues.append(f"Abstract too short (< {self.min_abstract_length} characters)")
            return score, issues
        
        if len(abstract) > self.max_abstract_length:
            issues.append(f"Abstract too long (> {self.max_abstract_length} characters)")
            return score, issues
        
        # å†…å®¹è´¨é‡æ£€æŸ¥
        score += 0.3  # åŸºç¡€åˆ†
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ€æœ¯æœ¯è¯­
        technical_terms = 0
        for term in ['method', 'approach', 'algorithm', 'model', 'analysis', 'system', 'technique']:
            if term.lower() in abstract.lower():
                technical_terms += 1
        
        if technical_terms >= 3:
            score += 0.3
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„ç ”ç©¶å†…å®¹æè¿°
        if any(phrase in abstract.lower() for phrase in ['this paper', 'this study', 'we present', 'we propose', 'this work']):
            score += 0.2
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœæè¿°
        if any(phrase in abstract.lower() for phrase in ['results show', 'experiments', 'evaluation', 'performance', 'accuracy']):
            score += 0.2
        
        return min(score, 1.0), issues
    
    def _validate_phm_relevance(self, paper: Dict[str, Any]) -> Tuple[float, List[str]]:
        """éªŒè¯ PHM ç›¸å…³æ€§"""
        
        # åˆå¹¶æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯è¿›è¡Œæ£€æŸ¥
        text_content = ' '.join([
            paper.get('title', ''),
            paper.get('abstract', ''),
            ' '.join(paper.get('keywords', []))
        ]).lower()
        
        issues = []
        score = 0.0
        
        if not text_content.strip():
            issues.append("No content available for PHM relevance check")
            return score, issues
        
        # æ£€æŸ¥æ ¸å¿ƒ PHM å…³é”®è¯
        core_matches = sum(1 for keyword in self.phm_keywords['core'] 
                          if keyword.lower() in text_content)
        
        # æ£€æŸ¥æŠ€æœ¯å…³é”®è¯
        technical_matches = sum(1 for keyword in self.phm_keywords['technical'] 
                               if keyword.lower() in text_content)
        
        # æ£€æŸ¥åº”ç”¨å…³é”®è¯
        app_matches = sum(1 for keyword in self.phm_keywords['applications'] 
                         if keyword.lower() in text_content)
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        if core_matches >= 1:
            score += 0.5
        if technical_matches >= 1:
            score += 0.3
        if app_matches >= 1:
            score += 0.2
        
        # å¦‚æœæ²¡æœ‰ä»»ä½• PHM ç›¸å…³è¯æ±‡ï¼Œæ ‡è®°ä¸ºä¸ç›¸å…³
        if core_matches == 0 and technical_matches == 0 and app_matches == 0:
            issues.append("Paper does not appear to be related to PHM (Prognostics and Health Management)")
        
        return min(score, 1.0), issues
    
    def _validate_source(self, paper: Dict[str, Any]) -> Tuple[float, List[str]]:
        """éªŒè¯è®ºæ–‡æ¥æº"""
        
        source = paper.get('source', '').lower()
        url = paper.get('url', '')
        
        issues = []
        score = 0.0
        
        # æ£€æŸ¥ URL æ˜¯å¦æ¥è‡ªå¯ä¿¡åŸŸå
        if url:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()
            
            # ç§»é™¤ www. å‰ç¼€
            if domain.startswith('www.'):
                domain = domain[4:]
            
            if any(trusted in domain for trusted in self.trusted_domains):
                score += 0.7
            else:
                issues.append(f"Source domain not in trusted list: {domain}")
        
        # æ£€æŸ¥æ¥æºæ ‡è¯†
        trusted_sources = ['arxiv', 'ieee', 'pubmed', 'google_scholar', 'semantic_scholar']
        if source in trusted_sources:
            score += 0.3
        
        return min(score, 1.0), issues
    
    def _generate_review_summary(self, review_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå®¡æŸ¥æ‘˜è¦"""
        
        total_papers = review_results['total_papers']
        approved_count = len(review_results['approved_papers'])
        rejected_count = len(review_results['rejected_papers'])
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        approved_scores = [p['review_score'] for p in review_results['approved_papers']]
        avg_approved_score = sum(approved_scores) / len(approved_scores) if approved_scores else 0
        
        # ç»Ÿè®¡æ‹’ç»åŸå› 
        rejection_reasons = {}
        for rejected in review_results['rejected_papers']:
            for reason in rejected['rejection_reasons']:
                rejection_reasons[reason] = rejection_reasons.get(reason, 0) + 1
        
        # ç»Ÿè®¡è­¦å‘Š
        warning_counts = {}
        for warning in review_results['warnings']:
            warning_counts[warning] = warning_counts.get(warning, 0) + 1
        
        summary = {
            'approval_rate': approved_count / total_papers if total_papers > 0 else 0,
            'average_approved_score': round(avg_approved_score, 2),
            'most_common_rejection_reasons': sorted(rejection_reasons.items(), 
                                                   key=lambda x: x[1], reverse=True)[:5],
            'most_common_warnings': sorted(warning_counts.items(), 
                                         key=lambda x: x[1], reverse=True)[:5],
            'quality_assessment': self._assess_overall_quality(approved_count, total_papers, avg_approved_score),
            'recommendations': self._generate_recommendations(review_results)
        }
        
        return summary
    
    def _assess_overall_quality(self, approved_count: int, total_count: int, avg_score: float) -> str:
        """è¯„ä¼°æ•´ä½“è´¨é‡"""
        
        approval_rate = approved_count / total_count if total_count > 0 else 0
        
        if approval_rate >= 0.8 and avg_score >= 0.8:
            return "Excellent - High approval rate with high-quality papers"
        elif approval_rate >= 0.6 and avg_score >= 0.7:
            return "Good - Most papers meet quality standards"
        elif approval_rate >= 0.4:
            return "Fair - Some quality issues detected"
        else:
            return "Poor - Significant quality concerns"
    
    def _generate_recommendations(self, review_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        recommendations = []
        
        total_papers = review_results['total_papers']
        rejected_count = len(review_results['rejected_papers'])
        
        # åŸºäºæ‹’ç»ç‡ç»™å‡ºå»ºè®®
        if rejected_count / total_papers > 0.5:
            recommendations.append("Consider improving paper discovery quality filters")
        
        # åŸºäºå¸¸è§é—®é¢˜ç»™å‡ºå»ºè®®
        common_issues = [reason for reason, count in review_results['review_summary']['most_common_rejection_reasons']]
        
        if "Missing abstract" in common_issues:
            recommendations.append("Ensure all papers have complete abstracts before submission")
        
        if "Invalid DOI format" in common_issues:
            recommendations.append("Validate DOI formats during paper extraction")
        
        if "Paper does not appear to be related to PHM" in common_issues:
            recommendations.append("Improve PHM relevance filtering in discovery phase")
        
        if not recommendations:
            recommendations.append("Paper quality looks good - continue current practices")
        
        return recommendations
    
    def _calculate_comprehensive_quality_score(self, paper: Dict[str, Any]) -> float:
        """
        è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ†
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            ç»¼åˆè´¨é‡è¯„åˆ† (0.0-1.0)
        """
        
        total_score = 0.0
        
        # 1. æœŸåˆŠ/ä¼šè®®å£°èª‰è¯„åˆ† (30%)
        venue_score = self._evaluate_venue_reputation(paper)
        total_score += venue_score * self.quality_weights['venue_reputation']
        
        # 2. PHMç›¸å…³æ€§è¯„åˆ† (25%)
        relevance_score = self._evaluate_phm_relevance_score(paper)
        total_score += relevance_score * self.quality_weights['phm_relevance']
        
        # 3. å†…å®¹è´¨é‡è¯„åˆ† (20%)
        content_score = self._evaluate_content_quality_score(paper)
        total_score += content_score * self.quality_weights['content_quality']
        
        # 4. ä½œè€…å¯ä¿¡åº¦è¯„åˆ† (15%)
        author_score = self._evaluate_author_credibility(paper)
        total_score += author_score * self.quality_weights['author_credibility']
        
        # 5. åˆ›æ–°æ€§å’Œå½±å“åŠ›è¯„åˆ† (10%)
        novelty_score = self._evaluate_novelty_impact(paper)
        total_score += novelty_score * self.quality_weights['novelty_impact']
        
        return min(total_score, 1.0)
    
    def _get_quality_breakdown(self, paper: Dict[str, Any]) -> Dict[str, float]:
        """è·å–è´¨é‡è¯„åˆ†æ˜ç»†"""
        
        return {
            'venue_reputation': self._evaluate_venue_reputation(paper),
            'phm_relevance': self._evaluate_phm_relevance_score(paper),
            'content_quality': self._evaluate_content_quality_score(paper),
            'author_credibility': self._evaluate_author_credibility(paper),
            'novelty_impact': self._evaluate_novelty_impact(paper)
        }
    
    def _evaluate_venue_reputation(self, paper: Dict[str, Any]) -> float:
        """è¯„ä¼°æœŸåˆŠ/ä¼šè®®å£°èª‰"""
        
        venue = paper.get('venue', paper.get('journal', paper.get('conference', '')))
        source = paper.get('source', '').lower()
        
        # æ ¹æ®æœŸåˆŠåç§°ç›´æ¥åŒ¹é…
        for known_venue, score in self.venue_scores.items():
            if known_venue.lower() in venue.lower():
                return score
        
        # æ ¹æ®æ¥æºç±»å‹è¯„åˆ†
        if 'arxiv' in source:
            return self.venue_scores['arXiv']
        elif 'ieee' in source:
            return 0.85  # IEEE å¹³å‡åˆ†
        elif any(domain in source for domain in ['springer', 'elsevier', 'acm']):
            return 0.75
        else:
            return self.venue_scores['unknown']
    
    def _evaluate_phm_relevance_score(self, paper: Dict[str, Any]) -> float:
        """è¯„ä¼°PHMç›¸å…³æ€§è¯„åˆ†"""
        
        title = paper.get('title', '').lower()
        abstract = paper.get('abstract', '').lower()
        full_text = f"{title} {abstract}"
        
        # ç»Ÿè®¡å„ç±»å…³é”®è¯å‡ºç°æ¬¡æ•°
        core_matches = sum(1 for kw in self.phm_keywords['core'] if kw.lower() in full_text)
        tech_matches = sum(1 for kw in self.phm_keywords['technical'] if kw.lower() in full_text)
        app_matches = sum(1 for kw in self.phm_keywords['applications'] if kw.lower() in full_text)
        
        # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
        total_matches = core_matches * 3 + tech_matches * 2 + app_matches * 1
        max_possible = len(self.phm_keywords['core']) * 3 + len(self.phm_keywords['technical']) * 2 + len(self.phm_keywords['applications']) * 1
        
        relevance_score = min(total_matches / max_possible * 5, 1.0)  # æ”¾å¤§è¯„åˆ†ï¼Œæœ€é«˜ä¸º1.0
        
        # ç‰¹åˆ«å¥–åŠ±LLM+PHMçš„ç»“åˆ
        if any(llm_term in full_text for llm_term in ['large language model', 'llm', 'chatgpt', 'transformer']):
            relevance_score = min(relevance_score + 0.2, 1.0)
        
        return relevance_score
    
    def _evaluate_content_quality_score(self, paper: Dict[str, Any]) -> float:
        """è¯„ä¼°å†…å®¹è´¨é‡è¯„åˆ†"""
        
        score = 0.0
        
        # æ‘˜è¦è´¨é‡ (40%)
        abstract = paper.get('abstract', '')
        if abstract:
            abstract_len = len(abstract)
            if 150 <= abstract_len <= 1500:  # åˆç†é•¿åº¦
                score += 0.4
            elif 100 <= abstract_len < 150 or 1500 < abstract_len <= 2000:
                score += 0.3
            else:
                score += 0.1
        
        # æ ‡é¢˜è´¨é‡ (30%)
        title = paper.get('title', '')
        if title:
            title_len = len(title)
            if 20 <= title_len <= 120:  # åˆç†é•¿åº¦
                score += 0.3
                # æ ‡é¢˜åŒ…å«æŠ€æœ¯æœ¯è¯­çš„å¥–åŠ±
                if any(term in title.lower() for term in ['deep learning', 'machine learning', 'neural', 'llm']):
                    score += 0.1
        
        # ä½œè€…ä¿¡æ¯å®Œæ•´æ€§ (20%)
        authors = paper.get('authors', [])
        if authors and len(authors) >= 2:
            score += 0.2
        elif authors and len(authors) == 1:
            score += 0.1
        
        # DOIå­˜åœ¨æ€§ (10%)
        if paper.get('doi'):
            score += 0.1
        
        return min(score, 1.0)
    
    def _evaluate_author_credibility(self, paper: Dict[str, Any]) -> float:
        """è¯„ä¼°ä½œè€…å¯ä¿¡åº¦"""
        
        score = 0.0
        authors = paper.get('authors', [])
        
        if not authors:
            return 0.0
        
        # ä½œè€…æ•°é‡åˆç†æ€§ (30%)
        author_count = len(authors)
        if 2 <= author_count <= 6:  # åˆç†çš„åˆä½œå›¢é˜Ÿ
            score += 0.3
        elif author_count == 1:
            score += 0.2
        else:
            score += 0.1
        
        # ä½œè€…å§“åæ ¼å¼ (40%)
        valid_authors = 0
        for author in authors:
            if isinstance(author, str) and len(author.strip()) > 3:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆç†çš„å§“åæ ¼å¼
                if ',' in author or ' ' in author.strip():
                    valid_authors += 1
        
        if valid_authors > 0:
            score += 0.4 * (valid_authors / len(authors))
        
        # æœºæ„ä¿¡æ¯ (30%)
        # ç®€å•çš„æœºæ„ä¿¡æ¯æ£€æŸ¥
        text_content = f"{paper.get('title', '')} {paper.get('abstract', '')}"
        if any(inst in text_content.lower() for inst in ['university', 'institute', 'research', 'academy']):
            score += 0.3
        
        return min(score, 1.0)
    
    def _evaluate_novelty_impact(self, paper: Dict[str, Any]) -> float:
        """è¯„ä¼°åˆ›æ–°æ€§å’Œå½±å“åŠ›"""
        
        score = 0.0
        title = paper.get('title', '').lower()
        abstract = paper.get('abstract', '').lower()
        full_text = f"{title} {abstract}"
        
        # åˆ›æ–°æ€§å…³é”®è¯ (50%)
        innovation_keywords = [
            'novel', 'new', 'innovative', 'advanced', 'improved', 'enhanced',
            'state-of-the-art', 'cutting-edge', 'breakthrough', 'pioneering'
        ]
        
        innovation_matches = sum(1 for kw in innovation_keywords if kw in full_text)
        if innovation_matches > 0:
            score += min(innovation_matches * 0.1, 0.5)
        
        # æŠ€æœ¯å‰æ²¿æ€§ (30%)
        frontier_tech = [
            'large language model', 'transformer', 'attention mechanism',
            'foundation model', 'few-shot learning', 'zero-shot learning',
            'multimodal', 'graph neural network', 'knowledge graph'
        ]
        
        frontier_matches = sum(1 for tech in frontier_tech if tech in full_text)
        if frontier_matches > 0:
            score += min(frontier_matches * 0.15, 0.3)
        
        # å®éªŒéªŒè¯ (20%)
        experiment_keywords = [
            'experiment', 'evaluation', 'validation', 'dataset', 'benchmark',
            'comparison', 'performance', 'accuracy', 'results'
        ]
        
        exp_matches = sum(1 for kw in experiment_keywords if kw in full_text)
        if exp_matches > 0:
            score += min(exp_matches * 0.05, 0.2)
        
        return min(score, 1.0)


if __name__ == "__main__":
    # æµ‹è¯• Paper Review Agent
    config = {
        'paper_review_settings': {
            'strict_mode': True,
            'require_doi': False,
            'min_abstract_length': 100,
            'max_abstract_length': 2000
        }
    }
    
    agent = PaperReviewAgent(config)
    
    # æµ‹è¯•è®ºæ–‡
    test_papers = [
        {
            'title': 'Deep Learning for Bearing Fault Diagnosis in Prognostics and Health Management',
            'authors': ['Zhang, Wei', 'Liu, Ming'],
            'year': 2024,
            'abstract': 'This paper presents a novel deep learning approach for bearing fault diagnosis in prognostics and health management systems. The proposed CNN-LSTM method achieves high accuracy in fault classification and demonstrates superior performance compared to traditional methods. Experimental results on CWRU dataset show 97% accuracy.',
            'doi': '10.1016/j.ymssp.2024.111234',
            'source': 'ieee',
            'url': 'https://ieeexplore.ieee.org/document/test123'
        },
        {
            'title': 'Short title',  # è¿™ä¸ªä¼šè¢«æ‹’ç» - æ ‡é¢˜å¤ªçŸ­
            'authors': [],  # ç¼ºå°‘ä½œè€…
            'year': 2024,
            'abstract': 'Short abstract.',  # æ‘˜è¦å¤ªçŸ­
            'source': 'unknown'
        }
    ]
    
    test_input = {'papers': test_papers}
    
    print("Testing Paper Review Agent...")
    try:
        result = agent.process(test_input)
        print(f"\nğŸ“Š Review Results:")
        print(f"   Total papers: {result['total_papers']}")
        print(f"   Approved: {len(result['approved_papers'])}")
        print(f"   Rejected: {len(result['rejected_papers'])}")
        print(f"   Approval rate: {result['review_summary']['approval_rate']:.1%}")
        
        print(f"\nâœ… Approved papers:")
        for i, approved in enumerate(result['approved_papers'], 1):
            print(f"   {i}. {approved['paper']['title']} (Score: {approved['review_score']:.2f})")
        
        print(f"\nâŒ Rejected papers:")
        for i, rejected in enumerate(result['rejected_papers'], 1):
            print(f"   {i}. {rejected['paper'].get('title', 'Unknown')} - Reasons: {rejected['rejection_reasons']}")
        
    except Exception as e:
        print(f"Test failed: {e}")