#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Elsevier ScienceDirect APIå®¢æˆ·ç«¯
Elsevier ScienceDirect API Client

æä¾›Elsevier ScienceDirect APIçš„å®Œæ•´æ¥å…¥åŠŸèƒ½ï¼Œæ”¯æŒï¼š
- è®ºæ–‡æœç´¢å’Œæ£€ç´¢
- å…ƒæ•°æ®æå–
- å¼•ç”¨æ•°æ®è·å–
- è´¨é‡æŒ‡æ ‡è¯„ä¼°
"""

import requests
import json
import time
from typing import Dict, List, Any, Optional, Union
from urllib.parse import quote
from datetime import datetime

from .logging_config import get_logger
from .config import load_config


class ElsevierAPIError(Exception):
    """Elsevier APIä¸“ç”¨å¼‚å¸¸"""
    pass


class ElsevierClient:
    """
    Elsevier ScienceDirect APIå®¢æˆ·ç«¯
    
    æä¾›å¯¹Elsevier ScienceDirectæ•°æ®åº“çš„ç¨‹åºåŒ–è®¿é—®ï¼Œ
    æ”¯æŒè®ºæ–‡æœç´¢ã€å…ƒæ•°æ®æå–å’Œè´¨é‡è¯„ä¼°ã€‚
    """
    
    def __init__(self, config_path: str = "config.yaml", debug: bool = False):
        """
        åˆå§‹åŒ–Elsevierå®¢æˆ·ç«¯
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
        self.config = load_config(config_path)
        self.debug = debug
        self.logger = get_logger(__name__)
        
        # Elsevieré…ç½®
        elsevier_config = self.config.get('data_sources', {}).get('elsevier', {})
        self.api_key = elsevier_config.get('api_key', '')
        self.base_url = elsevier_config.get('base_url', 'https://api.elsevier.com')
        self.enabled = elsevier_config.get('enabled', False) and bool(self.api_key)
        
        if not self.enabled:
            self.logger.warning("Elsevier APIæœªå¯ç”¨æˆ–APIå¯†é’¥æœªé…ç½®")
            if debug:
                print("âš ï¸ Elsevier APIæœªå¯ç”¨ã€‚è¯·åœ¨config.yamlä¸­é…ç½®APIå¯†é’¥ã€‚")
                print("   è·å–APIå¯†é’¥: https://dev.elsevier.com/apikey/create")
        
        # APIè®¾ç½®
        self.settings = elsevier_config.get('settings', {})
        self.search_filters = elsevier_config.get('search_filters', {})
        
        # é€Ÿç‡é™åˆ¶é…ç½®
        api_config = self.config.get('api_configuration', {})
        self.rate_limit = api_config.get('rate_limits', {}).get('elsevier', 2)  # 2 req/sec
        self.timeout = api_config.get('timeout_seconds', 30)
        self.max_retries = api_config.get('max_retries', 3)
        self.retry_delay = api_config.get('retry_delay', 2)
        
        # è¯·æ±‚ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'X-ELS-APIKey': self.api_key,
            'Accept': 'application/json',
            'Content-Type': 'application/json',
            'User-Agent': api_config.get('user_agent', 'APPA/1.0')
        })
        
        # é€Ÿç‡æ§åˆ¶
        self._last_request_time = 0
        self._request_interval = 1.0 / self.rate_limit if self.rate_limit > 0 else 0
        
        if debug and self.enabled:
            print(f"âœ… Elsevier APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            print(f"   Base URL: {self.base_url}")
            print(f"   Rate Limit: {self.rate_limit} req/sec")
    
    def _rate_limit_wait(self):
        """æ‰§è¡Œé€Ÿç‡é™åˆ¶ç­‰å¾…"""
        if self._request_interval > 0:
            time_since_last = time.time() - self._last_request_time
            if time_since_last < self._request_interval:
                sleep_time = self._request_interval - time_since_last
                time.sleep(sleep_time)
        
        self._last_request_time = time.time()
    
    def _make_request(self, endpoint: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        å‘é€APIè¯·æ±‚
        
        Args:
            endpoint: APIç«¯ç‚¹
            params: è¯·æ±‚å‚æ•°
            
        Returns:
            Dict: APIå“åº”æ•°æ®
            
        Raises:
            ElsevierAPIError: APIè¯·æ±‚å¤±è´¥
        """
        if not self.enabled:
            raise ElsevierAPIError("Elsevier APIæœªå¯ç”¨æˆ–APIå¯†é’¥æœªé…ç½®")
        
        url = f"{self.base_url}/{endpoint.lstrip('/')}"
        params = params or {}
        
        for attempt in range(self.max_retries + 1):
            try:
                # é€Ÿç‡é™åˆ¶
                self._rate_limit_wait()
                
                if self.debug:
                    print(f"ğŸ” è¯·æ±‚Elsevier API: {url}")
                    print(f"   å‚æ•°: {params}")
                
                response = self.session.get(url, params=params, timeout=self.timeout)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 200:
                    data = response.json()
                    if self.debug:
                        print(f"âœ… APIè¯·æ±‚æˆåŠŸ: {response.status_code}")
                    return data
                
                elif response.status_code == 401:
                    raise ElsevierAPIError(f"APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ: {response.status_code}")
                
                elif response.status_code == 403:
                    raise ElsevierAPIError(f"è®¿é—®è¢«æ‹’ç»ï¼Œæ£€æŸ¥æƒé™: {response.status_code}")
                
                elif response.status_code == 429:
                    # é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                    wait_time = self.retry_delay * (2 ** attempt)
                    self.logger.warning(f"é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’")
                    if attempt < self.max_retries:
                        time.sleep(wait_time)
                        continue
                    else:
                        raise ElsevierAPIError(f"è¶…å‡ºé€Ÿç‡é™åˆ¶: {response.status_code}")
                
                else:
                    self.logger.warning(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                    if attempt < self.max_retries:
                        time.sleep(self.retry_delay)
                        continue
                    else:
                        raise ElsevierAPIError(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                
            except requests.RequestException as e:
                self.logger.error(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
                if attempt < self.max_retries:
                    time.sleep(self.retry_delay)
                    continue
                else:
                    raise ElsevierAPIError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    
    def test_connection(self) -> bool:
        """
        æµ‹è¯•APIè¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        if not self.enabled:
            return False
        
        try:
            # ä½¿ç”¨ç®€å•çš„æœç´¢æµ‹è¯•è¿æ¥
            result = self._make_request('/content/search/sciencedirect', {
                'query': 'prognostics',
                'count': 1
            })
            return 'search-results' in result
        
        except Exception as e:
            self.logger.error(f"Elsevier APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def search_papers(self, 
                     query: str,
                     max_results: int = 25,
                     year_range: Optional[tuple] = None,
                     subject_areas: Optional[List[str]] = None,
                     content_types: Optional[List[str]] = None,
                     open_access_only: bool = False) -> List[Dict[str, Any]]:
        """
        æœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            year_range: å¹´ä»½èŒƒå›´ (start_year, end_year)
            subject_areas: å­¦ç§‘é¢†åŸŸè¿‡æ»¤
            content_types: å†…å®¹ç±»å‹è¿‡æ»¤
            open_access_only: ä»…å¼€æ”¾è·å–è®ºæ–‡
            
        Returns:
            List[Dict]: è®ºæ–‡åˆ—è¡¨
        """
        if not self.enabled:
            self.logger.warning("Elsevier APIæœªå¯ç”¨ï¼Œè·³è¿‡æœç´¢")
            return []
        
        self.logger.info(f"æœç´¢Elsevierè®ºæ–‡: '{query}' (æœ€å¤§ {max_results} ç¯‡)")
        
        # æ„å»ºæœç´¢å‚æ•°
        params = {
            'query': query,
            'count': min(max_results, self.settings.get('max_results_per_request', 100)),
            'start': 0
        }
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        filters = []
        
        # å¹´ä»½è¿‡æ»¤
        if year_range:
            start_year, end_year = year_range
            filters.append(f'pub-date > {start_year - 1} AND pub-date < {end_year + 1}')
        
        # å­¦ç§‘é¢†åŸŸè¿‡æ»¤
        if subject_areas or self.search_filters.get('subject_areas'):
            areas = subject_areas or self.search_filters.get('subject_areas', [])
            area_filter = ' OR '.join([f'SUBJAREA({area})' for area in areas])
            filters.append(f'({area_filter})')
        
        # å†…å®¹ç±»å‹è¿‡æ»¤
        if content_types or self.search_filters.get('content_types'):
            types = content_types or self.search_filters.get('content_types', [])
            if 'journal' in types:
                filters.append('SRCTYPE(j)')
        
        # å¼€æ”¾è·å–è¿‡æ»¤
        if open_access_only or self.search_filters.get('open_access_only', False):
            filters.append('OPENACCESS(1)')
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if filters:
            params['query'] += ' AND ' + ' AND '.join(filters)
        
        papers = []
        processed_results = 0
        
        try:
            while processed_results < max_results:
                # æ›´æ–°èµ·å§‹ä½ç½®
                params['start'] = processed_results
                params['count'] = min(100, max_results - processed_results)
                
                # å‘é€è¯·æ±‚
                response = self._make_request('/content/search/sciencedirect', params)
                
                # è§£æç»“æœ
                search_results = response.get('search-results', {})
                entries = search_results.get('entry', [])
                
                if not entries:
                    break
                
                # å¤„ç†æ¯ä¸ªç»“æœ
                for entry in entries:
                    if len(papers) >= max_results:
                        break
                    
                    paper = self._parse_paper_entry(entry)
                    if paper:
                        papers.append(paper)
                
                processed_results += len(entries)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šç»“æœ
                total_results = int(search_results.get('opensearch:totalResults', 0))
                if processed_results >= total_results:
                    break
        
        except Exception as e:
            self.logger.error(f"Elsevieræœç´¢å¤±è´¥: {e}")
            raise ElsevierAPIError(f"æœç´¢å¤±è´¥: {e}")
        
        self.logger.info(f"Elsevieræœç´¢å®Œæˆ: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def _parse_paper_entry(self, entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        è§£æè®ºæ–‡æ¡ç›®
        
        Args:
            entry: Elsevier APIè¿”å›çš„è®ºæ–‡æ¡ç›®
            
        Returns:
            Dict: æ ‡å‡†åŒ–çš„è®ºæ–‡ä¿¡æ¯ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
        """
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            title = entry.get('dc:title', '')
            if not title:
                return None
            
            # DOIä¿¡æ¯
            doi = ''
            identifier = entry.get('dc:identifier', '')
            if identifier and identifier.startswith('DOI:'):
                doi = identifier.replace('DOI:', '')
            
            # ä½œè€…ä¿¡æ¯
            authors = []
            creator_info = entry.get('dc:creator', '')
            if creator_info:
                # è§£æä½œè€…å­—ç¬¦ä¸² "Author1; Author2; Author3"
                author_parts = creator_info.split(';')
                for author in author_parts:
                    author = author.strip()
                    if author:
                        authors.append(author)
            
            # æœŸåˆŠä¿¡æ¯
            venue = entry.get('prism:publicationName', '')
            
            # å‘è¡¨å¹´ä»½
            year = None
            cover_date = entry.get('prism:coverDate', '')
            if cover_date:
                try:
                    year = int(cover_date.split('-')[0])
                except ValueError:
                    pass
            
            # URLä¿¡æ¯
            urls = {}
            links = entry.get('link', [])
            if isinstance(links, list):
                for link in links:
                    if isinstance(link, dict):
                        rel = link.get('@rel', '')
                        href = link.get('@href', '')
                        if rel == 'scidir' and href:
                            urls['sciencedirect'] = href
                        elif rel == 'self' and href:
                            urls['api'] = href
            
            # æ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            abstract = entry.get('dc:description', '')
            
            # å¼•ç”¨æ¬¡æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            citations = 0
            cited_by = entry.get('citedby-count', '0')
            if cited_by and cited_by.isdigit():
                citations = int(cited_by)
            
            # æ„å»ºè®ºæ–‡å¯¹è±¡
            paper = {
                'id': self._generate_paper_id(title, authors, year),
                'title': title,
                'authors': authors,
                'year': year or datetime.now().year,
                'venue': venue,
                'venue_type': 'journal',  # Elsevierä¸»è¦æ˜¯æœŸåˆŠ
                'doi': doi,
                'abstract': abstract,
                'urls': urls,
                'quality_indicators': {
                    'citations': citations,
                    'peer_reviewed': True,  # ElsevieræœŸåˆŠéƒ½æ˜¯åŒè¡Œè¯„å®¡
                    'publisher': 'Elsevier',
                    'source': 'ScienceDirect'
                },
                'source_info': {
                    'database': 'Elsevier ScienceDirect',
                    'retrieved_at': datetime.now().isoformat(),
                    'api_entry': entry
                }
            }
            
            # PHMç›¸å…³æ€§è¯„ä¼°
            paper['phm_relevance_score'] = self._assess_phm_relevance(paper)
            
            return paper
        
        except Exception as e:
            self.logger.error(f"è§£æElsevierè®ºæ–‡æ¡ç›®å¤±è´¥: {e}")
            return None
    
    def _generate_paper_id(self, title: str, authors: List[str], year: Optional[int]) -> str:
        """ç”Ÿæˆè®ºæ–‡ID"""
        # è·å–ç¬¬ä¸€ä½œè€…å§“æ°
        first_author = "Unknown"
        if authors:
            author_parts = authors[0].split(',')
            if author_parts:
                first_author = author_parts[0].strip()
        
        # è·å–æ ‡é¢˜å…³é”®è¯
        title_words = title.lower().split()
        key_words = [w for w in title_words if len(w) > 3 and w.isalpha()][:2]
        title_part = '-'.join(key_words) if key_words else 'Paper'
        
        # ç”ŸæˆID
        year_str = str(year) if year else str(datetime.now().year)
        paper_id = f"{year_str}-Elsevier-{first_author}-{title_part}"
        
        # æ¸…ç†æ— æ•ˆå­—ç¬¦
        import re
        paper_id = re.sub(r'[^\w\-]', '', paper_id)
        
        return paper_id
    
    def _assess_phm_relevance(self, paper: Dict[str, Any]) -> float:
        """
        è¯„ä¼°è®ºæ–‡çš„PHMç›¸å…³æ€§
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯
            
        Returns:
            float: ç›¸å…³æ€§å¾—åˆ† (0.0-1.0)
        """
        title = paper.get('title', '').lower()
        abstract = paper.get('abstract', '').lower()
        venue = paper.get('venue', '').lower()
        
        text_content = f"{title} {abstract} {venue}"
        
        # PHMå…³é”®è¯æƒé‡
        phm_keywords = {
            # æ ¸å¿ƒæ¦‚å¿µ (é«˜æƒé‡)
            'prognostics': 1.0,
            'health management': 1.0,
            'fault diagnosis': 0.9,
            'predictive maintenance': 0.9,
            'remaining useful life': 1.0,
            'rul': 0.9,
            'condition monitoring': 0.8,
            'health monitoring': 0.8,
            
            # æŠ€æœ¯æ–¹æ³• (ä¸­ç­‰æƒé‡)
            'anomaly detection': 0.7,
            'failure prediction': 0.8,
            'degradation modeling': 0.7,
            'system reliability': 0.6,
            'sensor fusion': 0.6,
            
            # åº”ç”¨é¢†åŸŸ (ä¸­ç­‰æƒé‡)
            'bearing fault': 0.8,
            'gear fault': 0.7,
            'motor fault': 0.7,
            'turbine monitoring': 0.7,
            'machinery health': 0.8,
            
            # ç›¸å…³æ¦‚å¿µ (ä½æƒé‡)
            'machine learning': 0.4,
            'deep learning': 0.5,
            'artificial intelligence': 0.4,
            'signal processing': 0.5
        }
        
        # è®¡ç®—åŠ æƒç›¸å…³æ€§åˆ†æ•°
        total_score = 0.0
        max_possible_score = 0.0
        
        for keyword, weight in phm_keywords.items():
            if keyword in text_content:
                total_score += weight
            max_possible_score += weight
        
        # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        if max_possible_score > 0:
            relevance_score = min(total_score / max_possible_score * 2, 1.0)
        else:
            relevance_score = 0.0
        
        return round(relevance_score, 2)
    
    def get_article_details(self, doi: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯
        
        Args:
            doi: æ–‡ç« DOI
            
        Returns:
            Dict: è¯¦ç»†çš„æ–‡ç« ä¿¡æ¯
        """
        if not self.enabled or not doi:
            return None
        
        try:
            # ä½¿ç”¨Article Retrieval APIè·å–è¯¦ç»†ä¿¡æ¯
            endpoint = f'/content/article/doi/{doi}'
            response = self._make_request(endpoint)
            
            # è§£æå“åº”
            if 'full-text-retrieval-response' in response:
                article_data = response['full-text-retrieval-response']
                return self._parse_article_details(article_data)
            
            return None
        
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥ (DOI: {doi}): {e}")
            return None
    
    def _parse_article_details(self, article_data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£ææ–‡ç« è¯¦ç»†ä¿¡æ¯"""
        try:
            core_data = article_data.get('coredata', {})
            
            return {
                'title': core_data.get('dc:title', ''),
                'abstract': core_data.get('dc:description', ''),
                'keywords': core_data.get('dc:subject', []),
                'doi': core_data.get('prism:doi', ''),
                'publication_name': core_data.get('prism:publicationName', ''),
                'cover_date': core_data.get('prism:coverDate', ''),
                'page_range': core_data.get('prism:pageRange', ''),
                'volume': core_data.get('prism:volume', ''),
                'issue': core_data.get('prism:issueIdentifier', ''),
                'cited_by_count': int(core_data.get('citedby-count', 0)),
                'source_url': core_data.get('link', [{}])[0].get('@href', '')
            }
        
        except Exception as e:
            self.logger.error(f"è§£ææ–‡ç« è¯¦æƒ…å¤±è´¥: {e}")
            return {}
    
    def get_api_usage_stats(self) -> Dict[str, Any]:
        """
        è·å–APIä½¿ç”¨ç»Ÿè®¡ï¼ˆå¦‚æœæ”¯æŒï¼‰
        
        Returns:
            Dict: APIä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
        """
        # Elsevier APIé€šå¸¸ä¸æä¾›ä½¿ç”¨ç»Ÿè®¡ï¼Œè¿™é‡Œè¿”å›åŸºæœ¬ä¿¡æ¯
        return {
            'api_enabled': self.enabled,
            'api_key_configured': bool(self.api_key),
            'rate_limit': self.rate_limit,
            'base_url': self.base_url,
            'last_request_time': self._last_request_time
        }


def main():
    """æµ‹è¯•ç”¨ä¾‹"""
    import sys
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = ElsevierClient(debug=True)
    
    # æµ‹è¯•è¿æ¥
    print("ğŸ” æµ‹è¯•Elsevier APIè¿æ¥...")
    if client.test_connection():
        print("âœ… APIè¿æ¥æˆåŠŸï¼")
        
        # æµ‹è¯•æœç´¢
        print("\nğŸ” æµ‹è¯•è®ºæ–‡æœç´¢...")
        papers = client.search_papers(
            query="prognostics health management",
            max_results=5
        )
        
        print(f"âœ… æœç´¢å®Œæˆ: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        for i, paper in enumerate(papers, 1):
            print(f"\n{i}. {paper['title'][:60]}...")
            print(f"   ä½œè€…: {', '.join(paper['authors'][:2])}{'...' if len(paper['authors']) > 2 else ''}")
            print(f"   æœŸåˆŠ: {paper['venue']}")
            print(f"   å¹´ä»½: {paper['year']}")
            print(f"   PHMç›¸å…³æ€§: {paper['phm_relevance_score']}")
    
    else:
        print("âŒ APIè¿æ¥å¤±è´¥")
        print("è¯·æ£€æŸ¥:")
        print("1. config.yamlä¸­çš„APIå¯†é’¥é…ç½®")
        print("2. ç½‘ç»œè¿æ¥")
        print("3. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ")
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        stats = client.get_api_usage_stats()
        print(f"\né…ç½®çŠ¶æ€:")
        for key, value in stats.items():
            print(f"  {key}: {value}")


if __name__ == "__main__":
    main()